Since the distribution of clock synchronization was selected to be a normal distribution, the shape of the curve created by plotting the queue resemebles that of the \ac{CDF} of the distribution, noted $F(x)$.
A simple description of the traffic behavior can then be described in terms of that curve.
First, observe that when the queue hits a specific threshold, even if the queue is drained at an optimal rate, the $n$th queued packed will not be delivered in time:

\begin{equation}
Qsize - (DequeueRate * \delta t) \geq 0
\end{equation}

Where $\delta t$ is the deadline for the message to be delivered.
If the size of the queue exceeds the number of messages that can be delivered before $\delta t$ passes, some messages will not be delivered.
The size of the queue during the message bursts created by the DGI depends on the message complexity of the algorithm, the number of messages already in the queue, the other traffic on the network, and any replies that also have to be delivered in that interval.
Therefore, let $c$ represent the rate that traffic is generated by other processes.
Let $init_q$ represent the number of messages in the queue at the start of a burst. 
Let $init_m$ represent the number of messages sent in the beginning of the burst.
Let $resp$ represent the number of messages sent in response to the burst that must still be delivered before $\delta t$ passes.
We can then express $Qsize$ as two parts:

\begin{equation}
Qsize = Burst + Obligations
\end{equation}

Where $Burst$ takes the form of the \ac{CDF} for the normal distribution:

\begin{equation}
Burst = init_m * F(x)  
\end{equation}

\begin{equation}
Obligations = c * \delta t + init_q + resp
\end{equation}

From this we can derive the equation:

\begin{equation}
F(x) \geq \frac{DequeueRate * \delta t - c * \delta t - init_q - resp}{init_m}
\end{equation}

Solving for $F(x)$ gives a worst case estimate of the omission rate for a specific algorithmic or network circumstance.
